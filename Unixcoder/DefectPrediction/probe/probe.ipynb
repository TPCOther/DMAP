{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class ProbeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbeClassifier, self).__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.norm = nn.BatchNorm1d(768)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.out_proj = nn.Linear(768, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.norm(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_data = torch.load(\"./att/train_attentions_6th.pt\")\n",
    "\n",
    "label = [x[1] for x in train_data]\n",
    "pt_data = [x[0] for x in train_data]\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(label))\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "smote = TomekLinks(sampling_strategy='auto')\n",
    "pt_data, label = smote.fit_resample(pt_data, label)\n",
    "print(\"After SMOTE:\", np.bincount(label))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pt_data, label, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "print(y_train_tensor)\n",
    "print(torch.sum(y_train_tensor).item()/len(y_train_tensor))\n",
    "print((len(y_train_tensor) - torch.sum(y_train_tensor).item())/len(y_train_tensor))\n",
    "\n",
    "# 创建训练和测试的Dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "model = ProbeClassifier().to(device)\n",
    "\n",
    "# config = RobertaConfig.from_pretrained('microsoft/codebert-base')\n",
    "# config.num_labels = 1\n",
    "# modelx = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', config=config)\n",
    "\n",
    "# source_parameters_dense = modelx.classifier.dense.state_dict()\n",
    "# source_parameters_out_proj = modelx.classifier.out_proj.state_dict()\n",
    "\n",
    "# model.dense.load_state_dict(source_parameters_dense)\n",
    "# model.out_proj.load_state_dict(source_parameters_out_proj)\n",
    "\n",
    "# for param_source, param_target in zip(modelx.classifier.dense.parameters(), model.dense.parameters()):\n",
    "#     assert torch.equal(param_source, param_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "\n",
    "# 初始化损失跟踪列表\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        scores = model(data).squeeze()\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累积训练损失\n",
    "        total_train_loss += loss.item() * data.size(0)\n",
    "        total_train_samples += data.size(0)\n",
    "\n",
    "    # 计算每个epoch的平均训练损失\n",
    "    average_train_loss = total_train_loss / total_train_samples\n",
    "    train_losses.append(average_train_loss)\n",
    "    \n",
    "    # 测试损失计算\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    model.eval()  # 切换到评估模式\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            total_samples += data.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    test_losses.append(average_loss)\n",
    "\n",
    "    # 切换回训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 绘制训练和测试损失\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss by Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}: Train Loss: {train_losses[i]:.4f}, Test Loss: {test_losses[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model.eval()  # 设置模型为评估模式\n",
    "preds = []\n",
    "ylabels = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data).squeeze()\n",
    "        outputs = torch.softmax(outputs, dim=-1)\n",
    "        predicted = torch.argmax(outputs, dim=-1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        ylabels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(ylabels, preds)\n",
    "print(preds)\n",
    "print(cm)\n",
    "accuracy_per_class = cm.diagonal() / cm.sum(axis=0)\n",
    "\n",
    "# 打印结果\n",
    "for i, acc in enumerate(accuracy_per_class):\n",
    "    print(f\"Class {i}: {acc:.2f}\")\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ProbeClassifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
